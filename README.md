# ğŸ§  Market Research AI Agent

> **AI-powered market research agent** that generates comprehensive company reports in under 60 seconds â€” powered by NVIDIA Nemotron Nano on E2E Networks GPU infrastructure.

<p align="center">
  <img src="https://img.shields.io/badge/NVIDIA-Nemotron_Nano_30B-76B900?style=for-the-badge&logo=nvidia&logoColor=white" />
  <img src="https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white" />
  <img src="https://img.shields.io/badge/Tavily-AI_Search-5A67D8?style=for-the-badge" />
  <img src="https://img.shields.io/badge/E2E_Networks-A100_80GB-FF6B35?style=for-the-badge" />
</p>

---

## âœ¨ What It Does

Type a company name â†’ get a professional market research report with:

- ğŸ“Š **Company Overview** â€” products, services, market position
- ğŸ¯ **SWOT Analysis** â€” strengths, weaknesses, opportunities, threats
- ğŸ“ˆ **Market Trends** â€” 5-7 current industry trends with relevance ratings
- ğŸ† **Competitive Landscape** â€” competitor analysis and positioning
- ğŸ’¡ **Key Findings** â€” 10+ actionable insights
- ğŸ”— **40+ Sources** â€” all findings backed by real web sources
- ğŸ” **4-Tab Command Center** â€” Instant Web Searches, Web Crawling, Structured Extraction, and Deep Research pipelines.

**All running on your own GPU infrastructure. No OpenAI. No data leaving your cloud.**

---

## ğŸ—ï¸ Architecture

```
Your Laptop (Docker)                    E2E GPU Instance
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          â”‚           â”‚                       â”‚
â”‚  Frontend â”€â”€â–º Backend â”€â”€â”€â”€â”€ SSH â”€â”€â”€â”€â–ºâ”‚  vLLM Server          â”‚
â”‚  (Next.js)    (FastAPI)  â”‚  Tunnel   â”‚  Nemotron Nano 30B    â”‚
â”‚  :3000        :8080      â”‚           â”‚  :8000                â”‚
â”‚               â”‚          â”‚           â”‚                       â”‚
â”‚               â–¼          â”‚           â”‚  NVIDIA A100 80GB     â”‚
â”‚          Tavily API      â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚          (web search)    â”‚
â”‚               â”‚          â”‚
â”‚               â–¼          â”‚
â”‚          JSON Files      â”‚
â”‚     (cache + reports)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

1. **Search Agent** â€” Runs 4 strategic Tavily queries (overview, news, financial, competitors)
2. **Analyst Agent** â€” LLM generates SWOT analysis + market trends from search data
3. **Report Agent** â€” LLM compiles everything into a professional structured report

The pipeline runs in ~36 seconds, caches search results, and saves reports as JSON.

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Why |
|-----------|-----------|-----|
| **AI Model** | [NVIDIA Nemotron Nano 30B](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16) | 30B params, 3.5B active (MoE). Fast + smart. 1M token context. |
| **Model Serving** | vLLM | OpenAI-compatible API. High throughput. |
| **GPU** | NVIDIA A100 80GB (E2E Networks) | BF16 precision. Self-hosted in India. |
| **Backend** | FastAPI + Python 3.12 | Async, fast, auto-docs. |
| **Web Search** | Tavily API | AI-native search. Returns cleaned, LLM-ready content. |
| **Frontend** | Next.js + Tailwind CSS | Dark theme. Professional dashboard. |
| **Deployment** | Docker + SSH tunnel | Backend/frontend local, model on GPU cloud. |

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Node.js 18+
- Docker + docker-compose
- E2E Networks GPU instance with vLLM running

### 1. Clone & Setup

```bash
git clone https://github.com/namm9an/market-research-agent-.git
cd market-research-agent-
cp .env.example .env
# Edit .env with your Tavily API key
```

### 2. Start the SSH Tunnel (GPU connection)

```bash
ssh -L 8000:localhost:8000 root@<your-e2e-instance-ip>
```

### 3. Start the Backend

```bash
pip install -r requirements.txt
python -m uvicorn app.main:app --host 0.0.0.0 --port 8080
```

### 4. Test It

```bash
# Health check
curl http://localhost:8080/api/health | python3 -m json.tool

# Start research
curl -X POST http://localhost:8080/api/research \
  -H "Content-Type: application/json" \
  -d '{"query": "Zomato", "type": "company"}'

# Get results (replace YOUR_JOB_ID)
curl http://localhost:8080/api/research/YOUR_JOB_ID | python3 -m json.tool

# Export as markdown
curl "http://localhost:8080/api/research/YOUR_JOB_ID/export?format=md"
```

---

## ğŸ“¡ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/health` | Health check â€” vLLM + Tavily status |
| `POST` | `/api/research` | Start heavy 35-sec deep research job |
| `POST` | `/api/search` | Lightweight raw generic web search (Bypasses LLM) |
| `POST` | `/api/extract` | Target URLs to extract firmographics (Funding, ICP, Portfolio) |
| `POST` | `/api/crawl` | Target domains to extract full structured company profiles |
| `GET` | `/api/research/{job_id}` | Get job status + results |
| `GET` | `/api/research/{job_id}/export` | Export report (markdown/PDF/JSON) |
| `GET` | `/api/jobs` | List all research jobs |

### Example Response

```json
{
  "status": "completed",
  "duration_seconds": 36.5,
  "report": {
    "company_overview": "Zomato Ltd. is a leading Indian food-delivery...",
    "swot": {
      "strengths": ["58% market share...", "..."],
      "weaknesses": ["...", "..."],
      "opportunities": ["...", "..."],
      "threats": ["...", "..."]
    },
    "trends": [{"title": "...", "relevance": "high"}],
    "key_findings": ["10+ actionable insights"],
    "sources": ["40+ verified web sources"]
  }
}
```

---

## ğŸ“ Project Structure

```
market-research-agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                    # FastAPI entry point + API endpoints
â”‚   â”œâ”€â”€ config.py                  # Environment variables & settings
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py             # Pydantic models (SWOT, Report, etc.)
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ llm_service.py         # vLLM client (OpenAI-compatible)
â”‚   â”‚   â”œâ”€â”€ search_service.py      # Tavily 4-query search strategy
â”‚   â”‚   â””â”€â”€ research_engine.py     # Pipeline: search â†’ analyze â†’ compile
â”‚   â””â”€â”€ prompts/
â”‚       â””â”€â”€ templates.py           # SWOT, Trends, Report prompt templates
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cache/                     # Cached Tavily search results
â”‚   â”œâ”€â”€ reports/                   # Generated report JSON files
â”‚   â””â”€â”€ fallback/                  # Pre-loaded demo data
â”œâ”€â”€ setup_vllm.sh                  # GPU instance setup script
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## ğŸ”§ GPU Instance Setup

The model runs on an E2E Networks A100 80GB GPU instance:

```bash
# SSH into your E2E instance
ssh root@<instance-ip>

# Run the setup script
bash setup_vllm.sh
```

The script:
1. âœ… Checks GPU availability
2. âœ… Creates an isolated Python virtual environment
3. âœ… Installs vLLM + dependencies
4. âœ… Starts the model server in tmux
5. âœ… Waits for ready + runs a test

**Model**: `nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16` (~64GB VRAM)

---

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| **Research time** | ~36 seconds |
| **Sources gathered** | 40 per report |
| **SWOT points** | 4 per category |
| **Market trends** | 5-7 per report |
| **Key findings** | 10+ per report |
| **Model VRAM usage** | ~64GB (BF16) |
| **API credits per report** | ~8 Tavily credits |

---

## ğŸ¤ Built With

- **Model**: [NVIDIA Nemotron Nano 30B](https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16) â€” open-weight, MoE architecture
- **Infrastructure**: [E2E Networks](https://www.e2enetworks.com/) â€” Indian GPU cloud (A100 80GB)
- **Search**: [Tavily](https://tavily.com/) â€” AI-native search API
- **Serving**: [vLLM](https://github.com/vllm-project/vllm) â€” high-throughput LLM serving
